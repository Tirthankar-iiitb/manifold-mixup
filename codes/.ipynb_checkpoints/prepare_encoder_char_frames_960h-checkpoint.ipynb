{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f3ec63-b37e-4766-8440-b3bcc4acbc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forced alignment for char frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43ae7f5b-432c-4c23-bf68-75fefc848b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "from transformers import Wav2Vec2Processor\n",
    "from datasets import DatasetDict\n",
    "from dataclasses import dataclass\n",
    "import re,json\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53184a5c-b52f-457a-a77c-0389fbfd8017",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "SAMPLERATE=16000\n",
    "#VOCABJSON='IndicTimit_vocab.json'\n",
    "VOCABJSON='vocab_960h.json'\n",
    "PKLDIR='/home/ubuntu/manifold/new6/pkldata'\n",
    "DATASETDIR='/home/ubuntu/manifold/datasets'\n",
    "MODELDIR='/home/ubuntu/manifold/new6/models'\n",
    "\n",
    "vocabjson=f'{PKLDIR}/{VOCABJSON}'\n",
    "\n",
    "with open(vocabjson,'r') as f:\n",
    "    vocabs=json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5932ca9-387f-4ddd-8a2b-2750d2a9b441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trellis(emission, tokens, blank_id=0):\n",
    "    num_frame = emission.size(0)\n",
    "    num_tokens = len(tokens)\n",
    "    #print(f'num_tokens:{num_tokens}')\n",
    "    # Trellis has extra diemsions for both time axis and tokens.\n",
    "    # The extra dim for tokens represents <SoS> (start-of-sentence)\n",
    "    # The extra dim for time axis is for simplification of the code.\n",
    "    trellis = torch.empty((num_frame + 1, num_tokens + 1))\n",
    "    trellis[0, 0] = 0\n",
    "    trellis[1:, 0] = torch.cumsum(emission[:, 0], 0)\n",
    "    trellis[0, -num_tokens:] = -float(\"inf\")\n",
    "    trellis[-num_tokens:, 0] = float(\"inf\")\n",
    "\n",
    "    for t in range(num_frame):\n",
    "        trellis[t + 1, 1:] = torch.maximum(\n",
    "            # Score for staying at the same token\n",
    "            trellis[t, 1:] + emission[t, blank_id],\n",
    "            # Score for changing to the next token\n",
    "            trellis[t, :-1] + emission[t, tokens],\n",
    "        )            \n",
    "        \n",
    "    return trellis\n",
    "\n",
    "@dataclass\n",
    "class Point:\n",
    "    token_index: int\n",
    "    time_index: int\n",
    "    score: float\n",
    "\n",
    "\n",
    "def backtrack(trellis, emission, tokens, blank_id=0):\n",
    "    j = trellis.size(1) - 1\n",
    "    t_start = torch.argmax(trellis[:, j]).item()\n",
    "\n",
    "    path = []\n",
    "    for t in range(t_start, 0, -1):\n",
    "        stayed = trellis[t - 1, j] + emission[t - 1, blank_id]\n",
    "        changed = trellis[t - 1, j - 1] + emission[t - 1, tokens[j - 1]]\n",
    "        prob = emission[t - 1, tokens[j - 1] if changed > stayed else 0].exp().item()\n",
    "        path.append(Point(j - 1, t - 1, prob))\n",
    "\n",
    "        if changed > stayed:\n",
    "            j -= 1\n",
    "            if j == 0:\n",
    "                break\n",
    "    else:\n",
    "        raise ValueError(\"Failed to align\")\n",
    "    return path[::-1]\n",
    "\n",
    "    \n",
    "# Merge the labels\n",
    "@dataclass\n",
    "class Segment:\n",
    "    label: str\n",
    "    start: int\n",
    "    end: int\n",
    "    score: float\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.label}\\t({self.score:4.2f}): [{self.start:5d}, {self.end:5d})\"\n",
    "\n",
    "    @property\n",
    "    def length(self):\n",
    "        return self.end - self.start\n",
    "\n",
    "\n",
    "def merge_repeats(path,transcript):\n",
    "    i1, i2 = 0, 0\n",
    "    segments = []\n",
    "    while i1 < len(path):\n",
    "        while i2 < len(path) and path[i1].token_index == path[i2].token_index:\n",
    "            i2 += 1\n",
    "        score = sum(path[k].score for k in range(i1, i2)) / (i2 - i1)\n",
    "        segments.append(\n",
    "            Segment(\n",
    "                transcript[path[i1].token_index],\n",
    "                path[i1].time_index,\n",
    "                path[i2 - 1].time_index + 1,\n",
    "                score,\n",
    "            )\n",
    "        )\n",
    "        i1 = i2\n",
    "    return segments\n",
    "\n",
    "# Merge words\n",
    "def merge_words(segments, separator=\"|\"):\n",
    "    words = []\n",
    "    i1, i2 = 0, 0\n",
    "    while i1 < len(segments):\n",
    "        if i2 >= len(segments) or segments[i2].label == separator:\n",
    "            if i1 != i2:\n",
    "                segs = segments[i1:i2]\n",
    "                word = \"\".join([seg.label for seg in segs])\n",
    "                score = sum(seg.score * seg.length for seg in segs) / sum(seg.length for seg in segs)\n",
    "                words.append(Segment(word, segments[i1].start, segments[i2 - 1].end, score))\n",
    "            i1 = i2 + 1\n",
    "            i2 = i1\n",
    "        else:\n",
    "            i2 += 1\n",
    "    return words\n",
    "    \n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def plot_alignments(trellis, segments, word_segments, waveform, sampling_rate=16000):\n",
    "    trellis_with_path = trellis.clone()\n",
    "    for i, seg in enumerate(segments):\n",
    "        if seg.label != \"|\":\n",
    "            trellis_with_path[seg.start + 1 : seg.end + 1, i + 1] = float(\"nan\")\n",
    "\n",
    "    fig, [ax1, ax2] = plt.subplots(2, 1, figsize=(16, 9.5))\n",
    "\n",
    "    ax1.imshow(trellis_with_path[1:, 1:].T, origin=\"lower\")\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_yticks([])\n",
    "\n",
    "    for word in word_segments:\n",
    "        ax1.axvline(word.start - 0.5)\n",
    "        ax1.axvline(word.end - 0.5)\n",
    "\n",
    "    # The original waveform\n",
    "    ratio = waveform.size(0) / (trellis.size(0) - 1)\n",
    "    ax2.plot(waveform)\n",
    "    for word in word_segments:\n",
    "        x0 = ratio * word.start\n",
    "        x1 = ratio * word.end\n",
    "        #ax2.axvspan(x0, x1, alpha=0.1, edgecolor=\"blue\", ls='--',lw=2)\n",
    "        \n",
    "        ax2.axvline(x0,mfc='red',ls='--',lw=2)\n",
    "        ax2.axvline(x1,mfc='red',ls='--',lw=2)\n",
    "    xticks = ax2.get_xticks()\n",
    "    plt.xticks(xticks, xticks / sampling_rate)\n",
    "    ax2.set_xlabel(\"time [second]\")\n",
    "    ax2.set_yticks([])\n",
    "    ax2.set_ylim(-1.0, 1.0)\n",
    "    ax2.set_xlim(0, waveform.size(-1))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d79edbb-54e8-4a91-bcdf-bafc58f8bea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIN-train Completed...\n",
      "saved:/home/ubuntu/manifold/new6/pkldata/timit_support_set_HIN_960h_train_1.pkl\n",
      "--> val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIN-val Completed...\n",
      "saved:/home/ubuntu/manifold/new6/pkldata/timit_support_set_HIN_960h_val_1.pkl\n",
      "--> test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIN-test Completed...\n",
      "saved:/home/ubuntu/manifold/new6/pkldata/timit_support_set_HIN_960h_test_1.pkl\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "LANG_SET=['HIN']  # \n",
    "\n",
    "for LANG in LANG_SET:\n",
    "    DATASET=f'dataset_indic_timit_{LANG}'\n",
    "    # modelname=f'model_w2v2base_indic_timit_{LANG}'   \n",
    "    # MODELFILE=f'{MODELDIR}/{modelname}'\n",
    "    # print(f'Preparing pkldata for {LANG} using ref model {MODELFILE}...')\n",
    "    \n",
    "    for DATATYPE in ['train','val','test']:\n",
    "        \n",
    "        #PKLFILE=f'{PKLDIR}/timit_support_set_{LANG}_w2v2base_{DATATYPE}.pkl'\n",
    "        PKLFILE=f'{PKLDIR}/timit_support_set_{LANG}_960h_{DATATYPE}_1.pkl'\n",
    "\n",
    "        xxx_dataset=f'{DATASETDIR}/{DATASET}'\n",
    "        \n",
    "        dset=DatasetDict().load_from_disk(xxx_dataset)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            #model = Wav2Vec2ForCTC.from_pretrained(MODELFILE).to(device)\n",
    "\n",
    "            processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "            model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\").to(device)\n",
    "\n",
    "        #tokenizer = Wav2Vec2CTCTokenizer(vocabjson, unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
    "        #feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=False)\n",
    "        #processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "        charfeats={}\n",
    "        \n",
    "        checklen={}\n",
    "        for ky in vocabs.keys():\n",
    "            if ky in [\"<pad>\",\"<s>\",\"</s>\",\"<unk>\",\"'\"]: continue\n",
    "            checklen[ky]=False\n",
    "        lenupto=10000\n",
    "        cflag=False\n",
    "        num_rows=dset[DATATYPE].num_rows\n",
    "        print(f'--> {DATATYPE} : {num_rows} rows')\n",
    "        for idx in range(num_rows):\n",
    "            print(f'\\t--> {idx}', end='\\r')\n",
    "            #for ky in vocabs.keys():\n",
    "            for ky in checklen.keys():\n",
    "                if checklen[ky]:\n",
    "                    cflag=False\n",
    "                    continue\n",
    "                else:\n",
    "                    cflag=True\n",
    "                    break\n",
    "            if not cflag:\n",
    "                break\n",
    "            else:\n",
    "                waveform=torch.from_numpy(dset[DATATYPE][idx]['audio']['array'].astype(np.float32)).unsqueeze(0)\n",
    "                text=dset[DATATYPE]['text'][idx]\n",
    "                text_lst=text.split(' ')\n",
    "                ntext='|'.join(text_lst)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    emissions = model(waveform.to(device)).logits\n",
    "                emits=torch.log_softmax(emissions,dim=-1)\n",
    "                emits=emits[0].cpu().detach()\n",
    "                labels = processor(text=ntext.upper()).input_ids\n",
    "                \n",
    "                trellis=get_trellis(emits,labels)\n",
    "                path = backtrack(trellis, emits, labels)\n",
    "                segments = merge_repeats(path,ntext.upper())\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    #features\n",
    "                    fout=model.wav2vec2.feature_extractor(waveform.to(device))\n",
    "                    #projection\n",
    "                    fpout=model.wav2vec2.feature_projection(fout.permute(0,2,1))[0]\n",
    "                    #encoder out\n",
    "                    eout=model.wav2vec2.encoder(fpout).last_hidden_state\n",
    "\n",
    "                # prepare char-encfeat dict\n",
    "                for segm in segments:\n",
    "                    #print(segm.label,segm.start,segm.end)\n",
    "                    if not checklen[segm.label]:\n",
    "                        for fr in range(segm.start,segm.end):\n",
    "                            try:\n",
    "                                charfeats[segm.label].append(eout[:,fr,:])\n",
    "                            except:\n",
    "                                charfeats[segm.label]=[eout[:,fr,:]]\n",
    "                        if len(charfeats[segm.label])>lenupto:\n",
    "                            checklen[segm.label]=True\n",
    "\n",
    "\n",
    "        ncharfeats={}\n",
    "        charlens={}\n",
    "        for kys in charfeats.keys():\n",
    "            charlens[kys.lower()]=len(charfeats[kys])\n",
    "            feats=torch.stack(charfeats[kys]).squeeze(1).cpu()\n",
    "            ncharfeats[kys.lower()]=feats\n",
    "\n",
    "        with open(PKLFILE,'wb') as f:\n",
    "            pickle.dump((ncharfeats,charlens),f)\n",
    "\n",
    "        print(f'{LANG}-{DATATYPE} Completed...\\nsaved:{PKLFILE}')\n",
    "    print('------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7a9d5cf-f4e0-4bd2-a2bb-4ec8ba395292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'T': 10001, 'H': 10001, 'E': 10005, 'Y': 10003, '|': 10005, 'P': 9209, 'O': 10001, 'L': 10004, 'I': 10004, 'S': 10003, 'D': 10001, 'W': 8835, 'N': 10001, 'R': 10001, 'C': 10002, 'A': 10006, 'F': 9239, 'B': 6476, 'U': 10003, 'M': 10001, 'V': 3616, 'G': 9956, 'Z': 557, 'K': 3823, 'J': 666, 'X': 2075, 'Q': 345}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 27)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(charlens),len(charlens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "510c3244-69b8-4922-96bd-b5634712087b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['<pad>', '<s>', '</s>', '<unk>', '|', 'E', 'T', 'A', 'O', 'N', 'I', 'H', 'S', 'R', 'D', 'L', 'U', 'M', 'W', 'C', 'F', 'G', 'Y', 'P', 'B', 'V', 'K', \"'\", 'X', 'J', 'Q', 'Z'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6fe901-61e7-4280-9c17-d28c0e8b84eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2b75f7-6ece-423d-ae51-b9af0fd5bc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1171e723-00b9-4b10-baf6-e8f0005ec54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntext.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcd5ef6-e26e-4c58-b41a-644030ecad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels1 = processor(text=ntext.upper()).input_ids\n",
    "print(labels1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1827a5-1098-4e15-8e90-2b079b77d56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs_960h={\"<pad>\": 0, \"<s>\": 1, \"</s>\": 2, \"<unk>\": 3, \"|\": 4, \"E\": 5, \"T\": 6, \"A\": 7, \"O\": 8, \"N\": 9, \"I\": 10, \"H\": 11, \"S\": 12, \"R\": 13, \"D\": 14, \"L\": 15, \"U\": 16, \"M\": 17, \"W\": 18, \"C\": 19, \"F\": 20, \"G\": 21, \"Y\": 22, \"P\": 23, \"B\": 24, \"V\": 25, \"K\": 26, \"'\": 27, \"X\": 28, \"J\": 29, \"Q\": 30, \"Z\": 31}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea143a50-ce4e-4b6e-85ee-743032cab111",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
