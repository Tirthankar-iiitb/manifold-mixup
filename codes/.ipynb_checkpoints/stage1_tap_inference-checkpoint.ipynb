{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923a6258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tap stage1-mix/nomix layer outputs on test data for all accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2755fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference on stage1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1c77c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import transforms\n",
    "\n",
    "import pickle, os, random, io\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datetime\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912eae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader_chars_indic_timit import Dataloader_chars_indic_timit\n",
    "from dataloader_chars_indic_timit import stackup_inputs, collate_wrapper\n",
    "from dataloader_chars_indic_timit import device\n",
    "\n",
    "seed=25\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c73994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gethms(secs):\n",
    "    mm, ss = divmod(secs, 60)\n",
    "    hh, mm= divmod(mm, 60)\n",
    "    return f'{int(hh):02d}:{int(mm):02d}:{ss:.2f}'\n",
    "        \n",
    "def gethms_timedelta(td):\n",
    "    secs=td.total_seconds()\n",
    "    return gethms(secs)\n",
    "\n",
    "def mmd_linear(X, Y):\n",
    "    delta = X.mean(0) - Y.mean(0)\n",
    "    return delta.dot(delta.T)\n",
    "\n",
    "def mmd_category(x,y):\n",
    "    x1=x.contiguous().view(27,25,-1);y1=y.contiguous().view(27,25,-1)\n",
    "    ldim=x1.shape[-1]\n",
    "    a=[]\n",
    "    for i in range(27):\n",
    "        a.append(mmd_linear(x1[i],y1[i]))\n",
    "    a=torch.vstack(a)/ldim\n",
    "    return a.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89efe980",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cross_entropy_loss():\n",
    "    def __init__(self):\n",
    "        super(cross_entropy_loss, self).__init__()\n",
    "        self.eps=torch.tensor(np.finfo(float).eps)\n",
    "    \n",
    "    def loss(self,ypred,ytruth):\n",
    "        cross_entropy = -torch.mean(ytruth * torch.log(ypred + self.eps))\n",
    "        return cross_entropy\n",
    "\n",
    "class FCLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FCLayer, self).__init__()\n",
    "        self.fc=nn.Linear(input_size, output_size)\n",
    "        #self.dropout=nn.Dropout(p=0.3, inplace=False)\n",
    "        self.bnorm=nn.BatchNorm1d(output_size)\n",
    "        self.relu=nn.ReLU(inplace=True)\n",
    "        #self.residual = input_size == output_size    \n",
    "        self.residual = False\n",
    "                          \n",
    "    def ops(self,x):\n",
    "        x=self.fc(x)\n",
    "        #x=self.dropout(x)   # <-- new\n",
    "        x=self.bnorm(x.permute(0,2,1))\n",
    "        x=self.relu(x.permute(0,2,1))\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.residual:\n",
    "            return (self.ops(x) + x) / np.sqrt(2)\n",
    "        return self.ops(x)\n",
    "\n",
    "def mixup(x, shuffle, lam, i, j):\n",
    "    if shuffle is not None and lam is not None and i == j:\n",
    "        x = lam * x + (1 - lam) * x[:,shuffle,:]\n",
    "    return x\n",
    "    \n",
    "class Mixup_Model(nn.Module):\n",
    "    def __init__(self, num_classes, inputsize):\n",
    "        super(Mixup_Model, self).__init__()\n",
    "        self.sizes=[inputsize,512,128,32,2,32,128,512]\n",
    "        self.numlayers=len(self.sizes)-1\n",
    "        layers=[]\n",
    "        for i in range(len(self.sizes)-1):\n",
    "            layers.append(FCLayer(self.sizes[i],self.sizes[i+1]))\n",
    "        self.layers=nn.ModuleList(layers)\n",
    "        self.projection = nn.Linear(self.sizes[-1], num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if isinstance(x, list):\n",
    "            x, shuffle, lam = x\n",
    "        else:\n",
    "            shuffle = None\n",
    "            lam = None\n",
    "\n",
    "        taps=[]\n",
    "        # Decide which layer to mixup\n",
    "        j = np.random.randint(self.numlayers)\n",
    "        for k in range(self.numlayers):\n",
    "            x = mixup(x, shuffle, lam, k, j)\n",
    "            taps.append(x[0].cpu())\n",
    "            x = self.layers[k](x)\n",
    "        taps.append(x[0])\n",
    "        encout=x\n",
    "        x = self.projection(x)\n",
    "        taps.append(x[0])\n",
    "        \n",
    "        return x, encout, taps\n",
    "\n",
    "class Mixup_CTC_Model(nn.Module):\n",
    "    def __init__(self, num_classes, inputsize):\n",
    "        super(Mixup_CTC_Model, self).__init__()\n",
    "        \n",
    "        self.mixup_model=Mixup_Model(num_classes, inputsize)\n",
    "        encoutsize=self.mixup_model.sizes[-1]\n",
    "        self.ctc_block=FCLayer(encoutsize,num_classes+1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x,encout,_=self.mixup_model(x)\n",
    "        x=self.ctc_block(encout)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b1a3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_taps(mixmodel,nomixmodel):  #epoch, history=None\n",
    "    tapdata=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (XS,yS,Xlbls) in enumerate(test_dataloader):\n",
    "            bs=XS.shape[0]\n",
    "            XS=XS.contiguous().view(bs,-1,featsize).to(device)\n",
    "            break\n",
    "        _,_,taps1 = mixmodel([XS,None,None])\n",
    "        _,_,taps2 = nomixmodel([XS,None,None])\n",
    "\n",
    "    return taps1,taps2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed542e4f",
   "metadata": {},
   "source": [
    "### 'HIN' model train data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5c34cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "execdir='/root/manifold/experiments/new6b'\n",
    "pkldir=f'{execdir}/pkldata_new6b'\n",
    "mpath=f'{execdir}/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cdc11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_LANGS= ['HIN', 'TAM', 'BEN', 'MLY', 'MAR',  'KAN']\n",
    "\n",
    "classes=27\n",
    "kshot=25\n",
    "featsize=1024\n",
    "mixupsamples=100\n",
    "k=25; batch_size=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f740cce",
   "metadata": {},
   "source": [
    "## taps for different accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f53abc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage1modelpath_mix=f'{mpath}/model_stage1_mix.pth'\n",
    "stage1modelpath_nomix=f'{mpath}/model_stage1_nomix_best.pth'\n",
    "\n",
    "model_mix=Mixup_Model(classes,featsize).to(device)\n",
    "model_mix.load_state_dict(torch.load(stage1modelpath_mix, map_location=device))\n",
    "\n",
    "model_nomix=Mixup_Model(classes,featsize).to(device)\n",
    "model_nomix.load_state_dict(torch.load(stage1modelpath_nomix, map_location=device))\n",
    "\n",
    "for i,EVAL_LANG in enumerate(EVAL_LANGS):\n",
    "    evalsetup=f'{setup}_{EVAL_LANG}' \n",
    "    msg=f'Stage1 taps for: mix and nomix models'\n",
    "    print(f'{msg}...')\n",
    "\n",
    "    test_pkl=f'{pkldir}/timit_support_set_{EVAL_LANG}_960h_test_1.pkl'\n",
    "\n",
    "    vocabjson=f'{pkldir}/nvocabs_960h.json'\n",
    "    test_dataset=Dataloader_chars_indic_timit(kshot=k, support_pkl=test_pkl,                                   \n",
    "            vocabjson=vocabjson,  \n",
    "            samplingperepoch=mixupsamples, \n",
    "            transform=transforms.Compose([stackup_inputs(),]))\n",
    "\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, pin_memory=False,\n",
    "            collate_fn=collate_wrapper)\n",
    "    \n",
    "    taps_mix,taps_nomix = evaluate_taps(model_mix,model_nomix)\n",
    "\n",
    "    taps_file_mix=f'{pkldir}/test_taps_stage1_mix.pkl'\n",
    "    with open(taps_file_mix,'wb') as f:\n",
    "        pickle.dump(taps_mix,f)\n",
    "    a=taps_file_mix[taps_file_mix.rfind('/')+1:]\n",
    "    print(f'{EVAL_LANG} taps_mix pkl saved - {a}')\n",
    "    \n",
    "    taps_file_nomix=f'{pkldir}/test_taps_stage1_nomix.pkl'\n",
    "    with open(taps_file_nomix,'wb') as f:\n",
    "        pickle.dump(taps_nomix,f)\n",
    "    a=taps_file_nomix[taps_file_nomix.rfind('/')+1:]\n",
    "    print(f'{EVAL_LANG} taps_nomix pkl saved - {a}')\n",
    "    print()\n",
    "    \n",
    "## for train pkl\n",
    "print('--------')\n",
    "htaps_mix,htaps_nomix = get_HIN_train_taps(model_mix,model_nomix)\n",
    "\n",
    "htaps_file_mix=f'{pkldir}/train_taps_stage1_HIN_mix.pkl'\n",
    "with open(htaps_file_mix,'wb') as f:\n",
    "    pickle.dump(htaps_mix,f)\n",
    "a=htaps_file_mix[htaps_file_mix.rfind('/')+1:]\n",
    "print(f'HIN train taps_mix pkl saved - {a}')\n",
    "\n",
    "htaps_file_nomix=f'{pkldir}/train_taps_stage1_HIN_nomix.pkl'\n",
    "with open(htaps_file_nomix,'wb') as f:\n",
    "    pickle.dump(htaps_nomix,f)\n",
    "a=htaps_file_nomix[htaps_file_nomix.rfind('/')+1:]\n",
    "print(f'HIN train taps_nomix pkl saved - {a}')\n",
    "    \n",
    "print('\\nDone...')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13911747",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
