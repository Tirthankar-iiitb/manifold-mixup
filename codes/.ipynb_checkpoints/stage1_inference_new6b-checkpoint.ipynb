{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923a6258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stage-1\n",
    "# get loss-acc for mix-nomix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2755fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference on stage1 - new6b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a1c77c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import transforms\n",
    "\n",
    "import pickle, os, random, io\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datetime\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "912eae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader_chars_indic_timit import Dataloader_chars_indic_timit\n",
    "from dataloader_chars_indic_timit import stackup_inputs, collate_wrapper\n",
    "from dataloader_chars_indic_timit import device\n",
    "\n",
    "seed=25\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45c73994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gethms(secs):\n",
    "    mm, ss = divmod(secs, 60)\n",
    "    hh, mm= divmod(mm, 60)\n",
    "    return f'{int(hh):02d}:{int(mm):02d}:{ss:.2f}'\n",
    "        \n",
    "def gethms_timedelta(td):\n",
    "    secs=td.total_seconds()\n",
    "    return gethms(secs)\n",
    "\n",
    "def mmd_linear(X, Y):\n",
    "    delta = X.mean(0) - Y.mean(0)\n",
    "    return delta.dot(delta.T)\n",
    "\n",
    "def mmd_category(x,y):\n",
    "    x1=x.contiguous().view(27,25,-1);y1=y.contiguous().view(27,25,-1)\n",
    "    ldim=x1.shape[-1]\n",
    "    a=[]\n",
    "    for i in range(27):\n",
    "        a.append(mmd_linear(x1[i],y1[i]))\n",
    "    a=torch.vstack(a)/ldim\n",
    "    return a.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89efe980",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cross_entropy_loss():\n",
    "    def __init__(self):\n",
    "        super(cross_entropy_loss, self).__init__()\n",
    "        self.eps=torch.tensor(np.finfo(float).eps)\n",
    "    \n",
    "    def loss(self,ypred,ytruth):\n",
    "        cross_entropy = -torch.mean(ytruth * torch.log(ypred + self.eps))\n",
    "        return cross_entropy\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "class FCLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FCLayer, self).__init__()\n",
    "        self.fc=nn.Linear(input_size, output_size)\n",
    "        #self.dropout=nn.Dropout(p=0.3, inplace=False)\n",
    "        self.bnorm=nn.BatchNorm1d(output_size)\n",
    "        self.relu=nn.ReLU(inplace=True)\n",
    "        #self.residual = input_size == output_size    \n",
    "        self.residual = False\n",
    "                          \n",
    "    def ops(self,x):\n",
    "        x=self.fc(x)\n",
    "        #x=self.dropout(x)   # <-- new\n",
    "        x=self.bnorm(x.permute(0,2,1))\n",
    "        x=self.relu(x.permute(0,2,1))\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.residual:\n",
    "            return (self.ops(x) + x) / np.sqrt(2)\n",
    "        return self.ops(x)\n",
    "\n",
    "def mixup(x, shuffle, lam, i, j):\n",
    "    if shuffle is not None and lam is not None and i == j:\n",
    "        x = lam * x + (1 - lam) * x[:,shuffle,:]\n",
    "    return x\n",
    "    \n",
    "class Mixup_Model(nn.Module):\n",
    "    def __init__(self, num_classes, inputsize):\n",
    "        super(Mixup_Model, self).__init__()\n",
    "        self.sizes=[inputsize,512,128,32,2,32,128,512]\n",
    "        self.numlayers=len(self.sizes)-1\n",
    "        layers=[]\n",
    "        for i in range(len(self.sizes)-1):\n",
    "            layers.append(FCLayer(self.sizes[i],self.sizes[i+1]))\n",
    "        self.layers=nn.ModuleList(layers)\n",
    "        self.projection = nn.Linear(self.sizes[-1], num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if isinstance(x, list):\n",
    "            x, shuffle, lam = x\n",
    "        else:\n",
    "            shuffle = None\n",
    "            lam = None\n",
    "\n",
    "        taps=[]\n",
    "        # Decide which layer to mixup\n",
    "        j = np.random.randint(self.numlayers)\n",
    "        for k in range(self.numlayers):\n",
    "            x = mixup(x, shuffle, lam, k, j)\n",
    "            taps.append(x[0].cpu())\n",
    "            x = self.layers[k](x)\n",
    "        taps.append(x[0])\n",
    "        encout=x\n",
    "        x = self.projection(x)\n",
    "        taps.append(x[0])\n",
    "        \n",
    "        return x, encout, taps\n",
    "\n",
    "class Mixup_CTC_Model(nn.Module):\n",
    "    def __init__(self, num_classes, inputsize):\n",
    "        super(Mixup_CTC_Model, self).__init__()\n",
    "        \n",
    "        self.mixup_model=Mixup_Model(num_classes, inputsize)\n",
    "        encoutsize=self.mixup_model.sizes[-1]\n",
    "        self.ctc_block=FCLayer(encoutsize,num_classes+1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x,encout,_=self.mixup_model(x)\n",
    "        x=self.ctc_block(encout)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96f346a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():  #epoch, history=None\n",
    "    loss = []\n",
    "    acc = []\n",
    "    tapdata=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (XS,yS,Xlbls) in enumerate(test_dataloader):\n",
    "            bs=XS.shape[0]\n",
    "            XS=XS.contiguous().view(bs,-1,featsize).to(device)\n",
    "            output,_,taps = model([XS,None,None])\n",
    "            target=yS.to(device)\n",
    "            output=nn.Softmax(dim=2)(output)\n",
    "            loss.append(celoss.loss(output, target).cpu().data)\n",
    "            pred = output.data.max(2, keepdim=True)[1]\n",
    "            acc.append(pred.eq(target.max(2, keepdim=True)[1].data.view_as(pred)).cpu().float().mean().numpy())\n",
    "            \n",
    "            if batch_idx == (len(test_dataloader)-1):\n",
    "                tapdata.append((taps,target))\n",
    "            \n",
    "    \n",
    "    eval_loss=np.mean(loss)\n",
    "    accuracy = np.mean(acc)\n",
    "    \n",
    "    return eval_loss, accuracy, tapdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f5c34cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "execdir='/root/manifold/experiments/new6b'\n",
    "pkldir=f'{execdir}/pkldata_new6b'\n",
    "mpath=f'{execdir}/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50cdc11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_LANGS= ['HIN', 'TAM', 'BEN', 'MLY', 'MAR',  'KAN']\n",
    "spe='10K'\n",
    "setup=f'{spe}_best_new6b'  #'1000K_new6b'\n",
    "\n",
    "classes=27\n",
    "kshot=25\n",
    "featsize=1024\n",
    "mixupsamples=100\n",
    "k=25; batch_size=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56acbd29",
   "metadata": {},
   "source": [
    "## test loss-acc for mix and nomix for different accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea64648f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage1 evaluation for: 1000K_10K_best_new6b_HIN mix_nomix models...\n",
      "HIN --> mix...\tnomix\n",
      "Stage1 evaluation for: 1000K_10K_best_new6b_TAM mix_nomix models...\n",
      "TAM --> mix...\tnomix\n",
      "Stage1 evaluation for: 1000K_10K_best_new6b_BEN mix_nomix models...\n",
      "BEN --> mix...\tnomix\n",
      "Stage1 evaluation for: 1000K_10K_best_new6b_MLY mix_nomix models...\n",
      "MLY --> mix...\tnomix\n",
      "Stage1 evaluation for: 1000K_10K_best_new6b_MAR mix_nomix models...\n",
      "MAR --> mix...\tnomix\n",
      "Stage1 evaluation for: 1000K_10K_best_new6b_KAN mix_nomix models...\n",
      "KAN --> mix...\tnomix\n",
      "\n",
      "loss acc evaluations saved in: plots/loss_acc_inference_1000K_10K_best_new6b.csv\n",
      "\n",
      "Done...\n"
     ]
    }
   ],
   "source": [
    "spe_nomix='10K'\n",
    "spe_mix='1000K'\n",
    "setup_nomix=f'{spe_nomix}_best'\n",
    "setup_mix=f'{spe_mix}'\n",
    "test_setup=f'{setup_mix}_{setup_nomix}_new6b'\n",
    "\n",
    "history=pd.DataFrame()\n",
    "celoss=cross_entropy_loss()\n",
    "\n",
    "for i,EVAL_LANG in enumerate(EVAL_LANGS):\n",
    "    evalsetup=f'{test_setup}_{EVAL_LANG}' \n",
    "    msg=f'Stage1 evaluation for: {evalsetup} mix_nomix models'\n",
    "    print(f'{msg}...')\n",
    "\n",
    "    test_pkl=f'{pkldir}/timit_support_set_{EVAL_LANG}_960h_test_1.pkl'\n",
    "    vocabjson=f'{pkldir}/nvocabs_960h.json'\n",
    "    test_dataset=Dataloader_chars_indic_timit(kshot=k, support_pkl=test_pkl,                                   \n",
    "            vocabjson=vocabjson,  \n",
    "            samplingperepoch=mixupsamples, \n",
    "            transform=transforms.Compose([stackup_inputs(),]))\n",
    "\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, pin_memory=False,\n",
    "            collate_fn=collate_wrapper)\n",
    "    \n",
    "    stage1modelpath_mix=f'{mpath}/model_stage1_HIN_mix_{spe_mix}_new6b.pth'\n",
    "    stage1modelpath_nomix=f'{mpath}/model_stage1_HIN_nomix_new6b_{spe_nomix}_best.pth'\n",
    "    \n",
    "    for modeltype in ['mix', 'nomix']:\n",
    "        if modeltype=='mix':\n",
    "            print(f'{EVAL_LANG} --> {modeltype}...',end='\\t')\n",
    "            stage1modelpath=stage1modelpath_mix\n",
    "            spe=spe_mix\n",
    "        else:\n",
    "            print(f'{modeltype}...')\n",
    "            stage1modelpath=stage1modelpath_nomix\n",
    "            spe=spe_nomix\n",
    "            \n",
    "        model=Mixup_Model(classes,featsize).to(device)\n",
    "        model.load_state_dict(torch.load(stage1modelpath, map_location=device))\n",
    "\n",
    "        loss, acc, _ = evaluate()\n",
    "        \n",
    "        if history is not None:\n",
    "            history.loc[i, 'testlang'] = EVAL_LANG\n",
    "            history.loc[i, f'{spe}_{modeltype}_loss'] = f'{loss*100:.6f}'\n",
    "            history.loc[i, f'{spe}_{modeltype}_acc'] = f'{acc*100:.6f}'       \n",
    "        \n",
    "loss_acc_fn=f'{execdir}/plots/loss_acc_inference_{test_setup}.csv'\n",
    "history.to_csv(loss_acc_fn, encoding='utf-8', index=False)\n",
    "print()\n",
    "a=loss_acc_fn[loss_acc_fn.rfind('/')+1:]\n",
    "print(f'loss acc evaluations saved in: plots/{a}')\n",
    "print('\\nDone...')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13911747",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
